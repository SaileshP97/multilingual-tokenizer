# Multilingual Tokenizer

This repository is used to **extend a bilingual tokenizer to a multilingual one** by integrating text corpora from multiple Indian languages. It includes a complete pipeline to download, clean, merge, and prepare datasets from Sangraha and Wikipedia. The pipeline also supports tokenizer training and model initialization for multilingual NLP tasks.

---

## ğŸ“ Directory Structure

```bash
.
â”œâ”€â”€ ai4bharat_sangraha/
â”‚   â”œâ”€â”€ download_data.sh          # Downloads Sangraha corpus
â”‚   â””â”€â”€ sangraha_data/            # Contains downloaded Sangraha `.parquet` files
â”‚
â”œâ”€â”€ wiki_dump/
â”‚   â”œâ”€â”€ download_wikidump.sh      # Downloads Wikipedia dumps in text form
â”‚   â”œâ”€â”€ covert_to_parquet.py      # Converts raw Wiki text into `.parquet` format
â”‚   â””â”€â”€ wiki_parquet/             # Contains processed Wiki `.parquet` files
â”‚
â”œâ”€â”€ training_data/                # Output directory for cleaned and merged training data
â”‚   â””â”€â”€ all_languages_merged.parquet  # Final combined corpus
â”‚
â”œâ”€â”€ merge_training_data.py        # Filters and merges Sangraha + Wiki dumps into cleaned `.parquet` files
â”œâ”€â”€ train_tokenizer.py            # Trains a tokenizer on the final text corpus
â”œâ”€â”€ initialize_model.py           # Initializes a model using the trained tokenizer
â”œâ”€â”€ run_pipeline.sh               # Main script to run the full pipeline
â””â”€â”€ README.md                     # Project documentation (this file)

